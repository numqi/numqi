{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convex hull approximation\n",
    "\n",
    "Convex hull [wiki-link](https://en.wikipedia.org/wiki/Convex_hull) is a general math concept: given a set of points (could be infinite, but let's assume finite for simplicity) in Euclidean plane $\\{a_1,a_2,\\cdots,a_m\\}\\subset \\mathbb{R}^n$, the convex hull is defined as\n",
    "\n",
    "$$ \\mathrm{conv}(\\{a\\}) = \\{ x: x=\\sum_i p_ia_i, p_i\\geq 0,\\sum_ip_i=1 \\}. $$\n",
    "\n",
    "For infinite set of points $A$, the definition should be the intersection of all convex sets containing $A$.\n",
    "\n",
    "In quantum information field, the density matrix set is the convex hull of all pure states\n",
    "\n",
    "$$ \\{\\rho\\}=\\mathrm{conv}\\left( \\{x\\in\\mathbb{C}^d: \\lVert x \\rVert_2=1\\} \\right). $$\n",
    "\n",
    "and the SEP set is the convex hull of all product states\n",
    "\n",
    "$$ \\mathrm{SEP}= \\mathrm{conv}\\left( \\{x\\otimes y: x\\in\\mathbb{C}^{d_A},y\\in\\mathbb{C}^{d_B}, \\lVert x \\rVert_2=\\lVert y \\rVert_2=1\\} \\right)$$\n",
    "\n",
    "As said in previous tutorials, the SEP set is difficult to describe and in this tutorial we will approximate it by convex hull of finite number of product states.\n",
    "\n",
    "references\n",
    "\n",
    "1. \"Detecting entanglement by pure bosonic extension\" [arxiv-link](https://arxiv.org/abs/2209.10934)\n",
    "2. Upper bounds for relative entropy of entanglement based on active learning [doi-link](https://doi.org/10.1088/2058-9565/abb412)\n",
    "3. A Separability-Entanglement Classifier via Machine Learning [doi-link](https://doi.org/10.1103%2Fphysreva.98.012315)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numqi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization\n",
    "\n",
    "In the CHA method, we are going to parametrize all SEP states according to the definition.\n",
    "\n",
    "$$ \\rho=\\sum_{i\\in[N]} p_i |\\psi_{A,i}\\psi_{B,i}\\rangle\\langle \\psi_{A,i}\\psi_{B,i}| $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ p_i\\geq 0, \\sum_{i\\in[N]} p_i=1, \\langle \\psi_{A,i}|\\psi_{A,i}\\rangle=\\langle \\psi_{B,i}|\\psi_{B,i}\\rangle=1. $$\n",
    "\n",
    "Importantly, (TODO-ref) proves that for $d_A\\times d_B$ bipartite system, at most $N=d_A^2d_B^2$. Similar discussion is also available on [physics-stackexchange](https://physics.stackexchange.com/q/399675/283720) What is the minimum number of separable pure states needed to decompose arbitrary separable states?\n",
    "\n",
    "To make this parametrization, we need to find function mapping the trainable parameters from Euclidean space to the space of density matrices $\\rho$ in SEP. As for the positive coefficients $p_i$, we can use the softplus and the normalization function to generate\n",
    "\n",
    "$$ \\mathrm{SoftPlus}(\\theta)=\\log(1+e^\\theta) $$\n",
    "\n",
    "$$ p_i=\\frac{\\mathrm{SoftPlus}(\\theta_i)}{\\sum_j\\mathrm{SoftPlus}(\\theta_j)} $$\n",
    "\n",
    "where $\\theta$ is a vector of length $N$. As for the pure states $\\psi_{A,i}$ and $\\psi_{B,i}$, we can use the following parametrization\n",
    "\n",
    "$$ |\\psi_{A}\\rangle=\\frac{\\theta^{(r)}+\\mathrm{i}\\theta^{(i)}}{\\lVert \\theta^{(r)}+\\mathrm{i}\\theta^{(i)}\\rVert_2} $$\n",
    "\n",
    "where the real part $\\theta^{(r)}$ and the imaginary part $\\theta^{(i)}$ are all vectors of length $d_A$. Similarly, we can parametrize $|\\psi_{B}\\rangle$ with $\\theta^{(r)}$ and $\\theta^{(i)}$ of length $d_B$. Put all these together, we have the function mapping:\n",
    "\n",
    "$$ \\rho=f(\\theta)=\\sum_{i\\in[N]} p_i |\\psi_{A,i}\\psi_{B,i}\\rangle\\langle \\psi_{A,i}\\psi_{B,i}| $$\n",
    "\n",
    "with the number of trainable parameters $N(2d_A+2d_B+1)$.\n",
    "\n",
    "Let's experiment with the CHA model built in `numqi.entangle` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimA = 3\n",
    "dimB = 4\n",
    "\n",
    "model_cha = numqi.entangle.AutodiffCHAREE((dimA, dimB), num_state=2*dimA*dimB, distance_kind='ree')\n",
    "\n",
    "print('trainable parameters shape')\n",
    "for key,value in model_cha.named_parameters():\n",
    "    print(f'{key}:, {value.shape}')\n",
    "\n",
    "print('parametrized separable density matrix:')\n",
    "model_cha.set_dm_target(numqi.random.rand_density_matrix(dimA*dimB)) #just for test\n",
    "loss = model_cha()\n",
    "rho_cha = model_cha.dm_sep_torch.numpy()\n",
    "print('eigenvalue:', np.round(np.linalg.eigvalsh(rho_cha), 3))\n",
    "print('trace:', np.trace(rho_cha).real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we print out the shape of parameters $\\theta$ and the parametrized separable density matrix $\\rho$. We verify that $\\rho$ is indeed a density matrix (all eigenvalues are non-negative and the trace is 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Different tasks will use different loss function. For now, let's focus on find the \"closest\" separable state $\\sigma$ for a given density matrix $\\rho$.\n",
    "\n",
    "`distance_kind=\"gellmann\"` A naive idea is the Hilbert-Schmidt norm\n",
    "\n",
    "$$ \\mathcal{L}(\\sigma;\\rho)= \\lVert \\rho-\\sigma \\rVert_{\\mathrm{HS}}=\\sum_{ij}|\\rho_{ij}-\\sigma_{ij}|^2 $$\n",
    "\n",
    "$$ \\min_{\\sigma\\in\\mathrm{CHA}} \\mathcal{L}(\\sigma;\\rho) $$\n",
    "\n",
    "where we use $\\mathrm{CHA}$ to denote the parametrization above and we will see later that CHA is a good approximation of the $\\mathrm{SEP}$ set. Moreover, this \"distance\" is equivalent to the Euclidean distance in the vectorization of density matrix $\\vec{\\rho}$ (see [tutorial/gellmann](../gellmann.ipynb) for more details). Apparently, when the loss function is optimized to zero, we have $\\rho=\\sigma$ and $\\sigma$ is separable which means that we find a separable decomposition for $\\rho$. However, this loss function is not convex (with respect to the parameters) and there is no guarantee that the global minimum can be find. It should be noted that this optimized value is NOT entanglement measure as it could increase when we apply local operations and classical communication (LOCC) on $\\rho$ which violate the requirement of the entanglement measure.\n",
    "\n",
    "`distance_kind=\"ree\"` Another commonly used loss function is the quantum relative entropy [wiki/quantum-relative-entropy](https://en.wikipedia.org/wiki/Quantum_relative_entropy)\n",
    "\n",
    "$$ \\mathcal{L}(\\sigma;\\rho)=\\mathrm{Tr}[\\rho\\log\\rho-\\rho\\log\\sigma] $$\n",
    "\n",
    "$$ \\min_{\\sigma\\in\\mathrm{CHA}}\\mathcal{L}(\\sigma;\\rho) $$\n",
    "\n",
    "$\\mathcal{L}$ is nonnegative and it's zero if and only if $\\rho=\\sigma$. However, quantum relative entropy is not a distance function as it's asymmetrical with respect to $\\rho$ and $\\sigma$. Most importantly, it's jointly convex with respect to both $\\rho$ and $\\sigma$, so we will see some semi-definite programming (SDP) to solve some objective function in the later tutorials. However, it's not convex with respect to our parametrization $\\theta$ and we have to use non-convex optimization (gradient-based optimization) to solve it. The optimized (minimized) value is called relative entropy of entanglement (REE).\n",
    "\n",
    "Let's verify some analytical results of REE, e.g. Werner state and Isotropic state (TODO-link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d=3) 11 seconds\n",
    "# (d=4) 25 seconds\n",
    "dim = 3\n",
    "alpha_list = np.linspace(0, 1, 100, endpoint=False) # alpha=1 is unstable for matrix logarithm\n",
    "dm_target_list = [numqi.state.Werner(dim, x) for x in alpha_list]\n",
    "\n",
    "ree_analytical = np.array([numqi.state.get_Werner_ree(dim, x) for x in alpha_list])\n",
    "\n",
    "model = numqi.entangle.AutodiffCHAREE((dim, dim), distance_kind='ree')\n",
    "ree_cha = []\n",
    "for dm_target_i in tqdm(dm_target_list):\n",
    "    model.set_dm_target(dm_target_i)\n",
    "    tmp0 = numqi.optimize.minimize(model, theta0='uniform', tol=1e-12, num_repeat=1, print_every_round=0).fun\n",
    "    ree_cha.append(tmp0)\n",
    "ree_cha = np.array(ree_cha)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha_list, ree_analytical, \"x\", label=\"analytical\")\n",
    "ax.plot(alpha_list, ree_cha, label=\"CHA\")\n",
    "ax.set_xlim(min(alpha_list), max(alpha_list))\n",
    "ax.set_ylim(1e-13, 1)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\beta$\")\n",
    "ax.set_ylabel(\"REE\")\n",
    "ax.set_title(f'Werner({dim})')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "alpha_list = np.linspace(0, 1, 100, endpoint=False) # alpha=1 is unstable for matrix logarithm\n",
    "dm_target_list = [numqi.state.Isotropic(dim, x) for x in alpha_list]\n",
    "\n",
    "ree_analytical = np.array([numqi.state.get_Isotropic_ree(dim, x) for x in alpha_list])\n",
    "\n",
    "model = numqi.entangle.AutodiffCHAREE((dim, dim), distance_kind='ree')\n",
    "ree_cha = [] #about 11 seconds\n",
    "for dm_target_i in tqdm(dm_target_list):\n",
    "    model.set_dm_target(dm_target_i)\n",
    "    tmp0 = numqi.optimize.minimize(model, theta0='uniform', tol=1e-12, num_repeat=1, print_every_round=0).fun\n",
    "    ree_cha.append(tmp0)\n",
    "ree_cha = np.array(ree_cha)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha_list, ree_analytical, \"x\", label=\"analytical\")\n",
    "ax.plot(alpha_list, ree_cha, label=\"CHA\")\n",
    "ax.set_xlim(min(alpha_list), max(alpha_list))\n",
    "ax.set_ylim(1e-13, 1)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\beta$\")\n",
    "ax.set_ylabel(\"REE\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(f\"Isotropic({dim})\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCHA\n",
    "\n",
    "Convex Hull Approximation with Bagging (BCHA) from paper \"A Separability-Entanglement Classifier via Machine Learning\" [doi-link](https://doi.org/10.1103%2Fphysreva.98.012315). It's a method for finding the boundary, especially the boundary in UPB/BES direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm0 = numqi.entangle.load_upb('tiles', return_bes=True)[1]\n",
    "beta = numqi.entangle.CHABoundaryBagging((3,3)).solve(dm0, maxiter=150, use_tqdm=False)\n",
    "print(f'beta={beta}') #0.2279211623566359\n",
    "alpha = beta/numqi.gellmann.dm_to_gellmann_norm(dm0)\n",
    "print(f'alpha={alpha}') #0.8647 https://arxiv.org/abs/1705.01523\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
